{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search Mini Problem Set\n",
    "\n",
    "Welcome to Starfleet Academy! You are headed to your first class in Game Tactics, a necessary prerequisite if you want to qualify for the officer training track. The teacher of Game Tactics is Stonn, a notoriously xenophobic Vulcan who has never forgotten his humiliation at the hands of a human and a halfbreed. His prejudice is the only obstacle between you and the yellow uniform, but as long as you keep your head down it should only be a minor ordeal. You enter the classroom, seeing students putting away their books and a clock that reads 10:00. Daylight savings time has struck again!\n",
    "\n",
    "\"Cadet, since you finally deigned to grace us with your presence, could you show me your no doubt unparalleled primate intelligence by playing me in a simple game of Connect Four?\"\n",
    "\n",
    "There's no way you can beat him with your current skills. To win this challenge (complete this problem set), you must learn and employ the uniquely human tree search of MCTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what is a tree search?\n",
    "Trees are a special case of the graph problems previously seen in class. For example, consider Tic Tac Toe. From the starting blank board, each choice of where to draw an X is a possible future, followed by each choice of where to draw an O. A planner can look at this sprawl of futures and choose which action will most likely lead to victory.\n",
    "\n",
    "![Example 1](./example_1.png)\n",
    "\n",
    "## Breadth First Tree Search\n",
    "\n",
    "In a breadth first tree search the planner considers potential boards in order of depth. All boards that are one turn in the future are considered first, followed by the boards two turns away, until every potential board has been considered. The planner then chooses the best move to make based on whether the move will lead to victory or to defeat. In the following example, a breadth first search would identify that all other moves lead to a loss and instead pick the rightmost move.\n",
    "\n",
    "![Example 2](./example_2.png)\n",
    "\n",
    "## Monte Carlo Tree Search\n",
    "\n",
    "The problem with breadth first search is that it isn't at all clever. Tic Tac Toe is one of the simpler games in existence, but there are nearly three-hundred sixty thousand possible sets of moves for a BFS-based planner to consider. In a game with less constrained movement, like chess, this number exceeds the number of atoms in the known universe after looking only a couple of turns into the future. A Breadth First Search is too tied up with being logical and provably correct. Monte Carlo Tree Search leaps ahead to impulsively go where no search has gone before. In simpler terms, BFS is Spock while MCTS is Kirk.\n",
    "\n",
    "MCTS performs its search by repeatedly imagining play-throughs of the game or scenario, traveling down the entire branch of the game tree until it terminates. Based on how this play-through went, MCTS then updates the value of each node (move) involved based on whether it won or lost the playthrough. The Monte Carlo component comes from the fact that it chooses moves at random, not based on heuristics or visit count. This nondeterminism greatly increases the potential space it can explore even though its exploration will be much less rigorous.\n",
    "\n",
    "For example, let's look at the BFS tree above. The bad red moves have a high probability of loss because in 1/5 of the playthroughs X will instantly win. The correct, rightmost move will have a lower probability of a loss because X doesn't have this 1/5 chance of winning. MCTS will discard the red moves because of this higher loss probability, assigning them lower values whenever it selects them during a randomized play-through.\n",
    "\n",
    "## MCTS Algorithm\n",
    "\n",
    "[Check out the paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6145622&tag=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you write your very own MCTS bot, you need to get sped up on the API you will be using. But even before getting into the API, please run the following to import the API and some boilerplate code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import algo\n",
    "import sim\n",
    "import random\n",
    "from tests import *\n",
    "from game import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Board` Class\n",
    "\n",
    "The `Board` class is a template class that represents the state of a\n",
    "board at a single point in a game. We've created a `ConnectFourBoard`\n",
    "subclass that handles the mechanics for you. For any `Board` instance\n",
    "`board`, you have access to the following methods:\n",
    "\n",
    "**`board.get_legal_actions()`:** Returns a python set of `Action` class\n",
    "instances. Each element in the set is a valid action that can be applied\n",
    "to the `board` to create a new `Board` instance. See the `Action` API\n",
    "section below.\n",
    "\n",
    "**`board.is_terminal()`:** Returns `True` if `board` is an endgame board.\n",
    "Returns `False` otherwise.\n",
    "\n",
    "**`board.current_player_id()`:** Returns an integer that represents which player\n",
    "is expected to play next. For example, if this method returns 0, then the player\n",
    "who is the first player in some simulation of a game should be the next one to\n",
    "play an action. This is used internally in the `Simulation` class for bookkeeping,\n",
    "but you will need it when you do the backup step of the MCTS algorithm.\n",
    "\n",
    "**`board.reward_vector()`:** Returns a $n$-element tuple, where $n$ is the number\n",
    "of players, that contains the rewards earned by each player at this particular\n",
    "`board`. For Connect Four, $n=2$. Thus this method may return something like\n",
    "`(1,-1)`, meaning the player with ID 0 had a reward of 1, and the player with ID\n",
    "1 has reward -1.\n",
    "\n",
    "### The `Action` Class\n",
    "\n",
    "The `Action` class is for representing a single action that is meant to alter a\n",
    "`board`. We have written a `ConnectFourAction` subclass for you. Instances\n",
    "are hashable, so to check if two actions are the same, `hash(action1) == hash(action2)`\n",
    "can be used.\n",
    "For any `Action`\n",
    "instance `action`, you will only need the following method:\n",
    "\n",
    "**`action.apply(board)`:** Given a `Board` instance `board`, returns a new\n",
    "`Board` instance that represents the board after that action has been\n",
    "performed. If the `action` cannot be applied, an error is thrown.\n",
    "\n",
    "### The `Node` Class\n",
    "\n",
    "The `Node` class represents a single node in the MCTS tree that is constructed\n",
    "during each iteration of the algorithm. You will be interacting with this class\n",
    "the most. If you remember the algorithm, each node contains certain pieces of\n",
    "information that's associated with it. For any `Node` instance `node`, you have\n",
    "the following methods at you disposal:\n",
    "\n",
    "**`Node(board, action, parent)`**: The constructor takes three arguments.\n",
    "First, a `Board` instance `board` that the node will represent. Second,\n",
    "an `Action` instance 'action' that represents the incoming action that created\n",
    "`board`. Finally, a `Node` instance `parent`. For a root node, you would\n",
    "pass `None` in for both `action` and `parent`.\n",
    "\n",
    "**`node.get_board()`:** Returns the `Board` instance that `node` is representing.\n",
    "\n",
    "**`node.get_action()`:** Returns the incoming `Action` instance.\n",
    "\n",
    "**`node.get_parent()`:** Returns the parent `Node` instance.\n",
    "\n",
    "**`node.get_children()`:** Returns a list of `Node` instances that represent\n",
    "the children that have been expanded thus far.\n",
    "\n",
    "**`node.add_child(child)`:** Add a `Node` instance `child` to the list of expanded\n",
    "children under `node`.\n",
    "\n",
    "**`node.get_num_visits()`:** Returns the number of times `node` has been visited.\n",
    "\n",
    "**`node.get_player_id()`:** This just returns `board.current_player_id()`, where\n",
    "`board` is the board that was passed into the constructor.\n",
    "\n",
    "**`node.q_value()`:** Returns the total reward that the `node` has accumulated.\n",
    "This reward is contained in a variable `node.q` that you can access if it needs\n",
    "to be changed during the algorithm.\n",
    "\n",
    "**`node.visit()`:** Doesn't return anything, but increments the internal counter\n",
    "that keeps track of how many times the `node` has been visited.\n",
    "\n",
    "**`node.is_fully_expanded()`:** Return `True` is all children that can be reached\n",
    "from this node have been expanded. Returns `False` otherwise.\n",
    "\n",
    "**`node.value(c)`**: Returns the calculated UCT value for this node. The parameter\n",
    "`c` is the _exploration_ constant.\n",
    "\n",
    "### The `Player` Class\n",
    "\n",
    "The `Player` class represents, you guessed it, a player. You won't have to actually\n",
    "deal with this class at all in this problem set. It exists for running the\n",
    "simulation at the end. However, if you interested, you may look at `game.py`\n",
    "to see what methods are used.\n",
    "\n",
    "### The `Simulation` Class\n",
    "\n",
    "The `Simulation` class is used for setting up a simulation for multiple\n",
    "players to play a game. You again don't need to worry about this class, as it\n",
    "is for running the simulation at the end. Refer to `game.py` if your curious\n",
    "about how it works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set Code\n",
    "In the following parts, we will ask you to implement the Monte Carlo Tree Search algorithm to beat the bot. You will be implementing the pseudocode starting on page 10 of the [MCTS paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6145622&tag=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Policy\n",
    "The first step is to implement the default policy, which plays through an entire game session. It chooses actions at random, applying them to the board until the game is over. It then returns the reward vector of the finished board.\n",
    "\n",
    "<img src=\"default-policy.png\" alt=\"Default Policy Pseudocode\" width=\"350px\"/><br/>\n",
    "<center><em><small>Browne, et al.</small></em></center>\n",
    "\n",
    "**Note:** You can use `random.choice(my_list)` to select a random item from `my_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# randomly picking moves to reach the end game\n",
    "# Input: BOARD, the board that we want to start randomly picking moves\n",
    "# Output: the reward vector when the game terminates\n",
    "#######################################################################\n",
    "def default_policy(board):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_default_policy(default_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Policy\n",
    "\n",
    "<img src=\"tree-policy.png\" alt=\"Tree Policy Pseudocode\" width=\"250px\"/><br/>\n",
    "<center><em>Tree Policy Pseudocode (Browne, et al.)</em></center>\n",
    "\n",
    "\n",
    "The tree policy performs a depth-first search of the tree using `best_child` and `expand`. If it encounters an unexpanded node it will return the expanded child of that node. Otherwise, it continues its search in the best child of the current node.\n",
    "\n",
    "The `best_child` function find the best child node if a node is fully expanded. It also takes the exploitation constant as an argument.\n",
    "\n",
    "The `expand` function expands a node that has unexpanded children. It must get all current children of the node and all possible children of the node then add one of the possible children to the node. It should then return this newly added child."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Child\n",
    "<img src=\"best-child.png\" alt=\"Best Child Pseudocode\" width=\"400px\"/><br/>\n",
    "<center><em><small>Browne, et al.</small></em></center>\n",
    "\n",
    "**Note:** For convenience, we've implemented a function that returns the heuristic inside the max operator. Look at the function `node.value(c)` for the `NODE` class API and save yourself the headache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# get the best child from this node (using heuristic)\n",
    "# Input: NODE, the node we want to find the best child of\n",
    "#        C,    the exploitation constant\n",
    "# Output: the child node\n",
    "###########################################################\n",
    "def best_child(node, c):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_best_child(best_child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand\n",
    "<img src=\"expand.png\" alt=\"Expand Pseudocode\" width=\"400px\"/><br/>\n",
    "<center><em><small>Browne, et al.</small></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# expand a node since it is not fully expanded\n",
    "# Input: NODE, a node that want to be expanded\n",
    "# Output: the child node\n",
    "###########################################################\n",
    "def expand(node):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_expand(expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Policy\n",
    "<img src=\"tree-policy.png\" alt=\"Tree Policy Pseudocode\" width=\"250px\"/><br/>\n",
    "<center><em><small>Browne, et al.</small></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# heuristically search to the leaf level\n",
    "# Input: NODE, a node that want to search down \n",
    "#        C, the exploitation value\n",
    "# Output: the leaf node that we expand till\n",
    "##########################################################\n",
    "def tree_policy(node, c):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree_policy(tree_policy, expand, best_child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup\n",
    "Now its time to make a way to turn the reward from `default_policy` into the information that `tree_policy` needs. `backup` should take the terminal state and reward from `default_policy` and proceed up the tree, updating the nodes on its path based on the reward.\n",
    "\n",
    "<img src=\"backup.png\" alt=\"Backup Pseudocode\" width=\"250px\"/><br/>\n",
    "<center><em><small>Browne, et al.</small></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# reward update for the tree after one simulation\n",
    "# Input: NODE, the node that we want to backup from\n",
    "#        REWARD_VECTOR, the reward vector of this exploration\n",
    "# Output: nothing\n",
    "##############################################################\n",
    "def backup(node, reward_vector):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_backup(backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search (UCT)\n",
    "Time to put everything together! Keep running `tree_policy`, `default_policy`, and `backup` until you run out of time! Finally, return the best child's associated action.\n",
    "\n",
    "<img src=\"uct-search.png\" alt=\"Search Pseudocode\" width=\"300px\"/><br/>\n",
    "<center><em><small>Browne, et al.</small></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# monte carlo tree search algorithm using UCT heuristic\n",
    "# Input: BOARD, the current game board\n",
    "#        TIME_LIMIT, the time limit of the calculation in second\n",
    "# Output: class Action represents the best action to take\n",
    "####################################################################\n",
    "def uct(board, time_limit):\n",
    "    start_time = time.time()\n",
    "    root = Node(board, None, None)\n",
    "    while (time.time() - start_time) < time_limit:\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_uct(uct) # this test takes 15-30 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final Challenge\n",
    "Time to show Stonn the power of human ingenuity! Win at least 9 out of 10 games to triumph! [Suggested listening](https://www.youtube.com/watch?v=_dnZHea_TI0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.run_final_test(uct)"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
